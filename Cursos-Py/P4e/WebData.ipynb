{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "man_a = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for linea in man_a:\n",
    "    print(linea.decode().strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "man_a = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "contadores = dict()\n",
    "for linea in man_a:\n",
    "    palabras = linea.decode().split()\n",
    "    for palabra in palabras:\n",
    "        contadores[palabra] = contadores.get(palabra, 0) + 1\n",
    "print(contadores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "from bs4 import  BeautifulSoup\n",
    "\n",
    "url=input('Enter - ')\n",
    "html=urllib.request.urlopen(url).read()\n",
    "soup =BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# retieve all of the anchor tags\n",
    "tags = soup(\"a\")\n",
    "for  tag in tags:\n",
    "    print(tag.get('href', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL encontrada: http://www.dr-chuck.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "html_text = '<p>Please click <a href=\"http://www.dr-chuck.com\">here</a></p>'\n",
    "\n",
    "# Expresión regular para extraer la URL del atributo href\n",
    "pattern = r'href=\"(.+)\"'\n",
    "\n",
    "# Buscar la URL usando la regex\n",
    "match = re.search(pattern, html_text)\n",
    "\n",
    "# Si se encuentra una coincidencia, obtener el valor de la URL\n",
    "if match:\n",
    "    url = match.group(1)\n",
    "    print(\"URL encontrada:\", url)\n",
    "else:\n",
    "    print(\"URL no encontrada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome Mario Daniel Rodriguez Casallas from Using Python to Access Web Data\n",
    "\n",
    "Scraping Numbers from HTML using BeautifulSoup In this assignment you will write a Python program similar to http://www.py4e.com/code3/urllink2.py. The program will use urllib to read the HTML from the data files below, and parse the data, extracting numbers and compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "    Sample data: http://py4e-data.dr-chuck.net/comments_42.html (Sum=2553)\n",
    "    Actual data: http://py4e-data.dr-chuck.net/comments_1968702.html (Sum ends with 95)\n",
    "\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "\n",
    "Data Format\n",
    "\n",
    "The file is a table of names and comment counts. You can ignore most of the data in the file except for lines like the following:\n",
    "\n",
    "<tr><td>Modu</td><td><span class=\"comments\">90</span></td></tr>\n",
    "<tr><td>Kenzie</td><td><span class=\"comments\">88</span></td></tr>\n",
    "<tr><td>Hubert</td><td><span class=\"comments\">87</span></td></tr>\n",
    "\n",
    "You are to find all the <span> tags in the file and pull out the numbers from the tag and sum the numbers.\n",
    "\n",
    "Look at the sample code provided. It shows how to find all of a certain kind of tag, loop through the tags and extract the various aspects of the tags.\n",
    "\n",
    "...\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "   # Look at the parts of a tag\n",
    "   print 'TAG:',tag\n",
    "   print 'URL:',tag.get('href', None)\n",
    "   print 'Contents:',tag.contents[0]\n",
    "   print 'Attrs:',tag.attrs\n",
    "\n",
    "You need to adjust this code to look for span tags and pull out the text content of the span tag, convert them to integers and add them up to complete the assignment.\n",
    "\n",
    "Sample Execution\n",
    "\n",
    "$ python3 solution.py\n",
    "Enter - http://py4e-data.dr-chuck.net/comments_42.html\n",
    "Count 50\n",
    "Sum 2...\n",
    "\n",
    "Turning in the Assignment\n",
    "Enter the sum from the actual data and your Python code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 50\n",
      "Sum: 2595\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Solicitar la URL\n",
    "url = input(\"Enter URL: \")\n",
    "if len(url) < 1:\n",
    "#    url = \"hhttp://py4e-data.dr-chuck.net/comments_42.html\"\n",
    "    url = \"http://py4e-data.dr-chuck.net/comments_1968702.html\"\n",
    "\n",
    "# Leer el contenido HTML de la URL\n",
    "html = urllib.request.urlopen(url).read()\n",
    "\n",
    "# Analizar el HTML con BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Encontrar todas las etiquetas <span> con la clase 'comments'\n",
    "span_tags = soup.find_all('span', class_='comments')\n",
    "\n",
    "# Inicializar la suma\n",
    "total_sum = 0\n",
    "\n",
    "# Iterar sobre las etiquetas <span> y sumar los números\n",
    "for span in span_tags:\n",
    "    # Extraer el contenido de texto de la etiqueta <span>\n",
    "    number = int(span.text)\n",
    "    # Sumar el número\n",
    "    total_sum += number\n",
    "\n",
    "# Imprimir la suma total\n",
    "print(f\"Count: {len(span_tags)}\")\n",
    "print(f\"Sum: {total_sum}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome Mario Daniel Rodriguez Casallas from Using Python to Access Web Data\n",
    "\n",
    "Following Links in Python\n",
    "\n",
    "In this assignment you will write a Python program that expands on http://www.py4e.com/code3/urllinks.py. The program will use urllib to read the HTML from the data files below, extract the href= vaues from the anchor tags, scan for a tag that is in a particular position relative to the first name in the list, follow that link and repeat the process a number of times and report the last name you find.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the name for your testing and the other is the actual data you need to process for the assignment\n",
    "\n",
    "    Sample problem: Start at http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "    Find the link at position 3 (the first name is 1). Follow that link. Repeat this process 4 times. The answer is the last name that you retrieve.\n",
    "    Sequence of names: Fikret Montgomery Mhairade Butchi Anayah\n",
    "    Last name in sequence: Anayah\n",
    "    Actual problem: Start at: http://py4e-data.dr-chuck.net/known_by_Nasser.html\n",
    "    Find the link at position 18 (the first name is 1). Follow that link. Repeat this process 7 times. The answer is the last name that you retrieve.\n",
    "    Hint: The first character of the name of the last page that you will load is: T\n",
    "\n",
    "Strategy\n",
    "\n",
    "The web pages tweak the height between the links and hide the page after a few seconds to make it difficult for you to do the assignment without writing a Python program. But frankly with a little effort and patience you can overcome these attempts to make it a little harder to complete the assignment without writing a Python program. But that is not the point. The point is to write a clever Python program to solve the program.\n",
    "\n",
    "Sample execution\n",
    "\n",
    "Here is a sample execution of a solution:\n",
    "\n",
    "$ python3 solution.py\n",
    "Enter URL: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "Enter count: 4\n",
    "Enter position: 3\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Fikret.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Montgomery.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Mhairade.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Butchi.html\n",
    "Retrieving: http://py4e-data.dr-chuck.net/known_by_Anayah.html\n",
    "\n",
    "The answer to the assignment for this execution is \"Anayah\".\n",
    "\n",
    "Turning in the Assignment\n",
    "Enter the last name retrieved and your Python code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Samir.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Neshawn.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Calder.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Hanim.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Moore.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Taqwah.html\n",
      "Retrieving: http://py4e-data.dr-chuck.net/known_by_Tamar.html\n",
      "Last name retrieved: Tamar\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Solicitar la URL inicial, el número de repeticiones y la posición\n",
    "url = input(\"Enter URL: \")\n",
    "\n",
    "if len(url) < 1:\n",
    "    url = \"http://py4e-data.dr-chuck.net/known_by_Nasser.html\"\n",
    "\n",
    "count = int(input(\"Enter count: \"))\n",
    "\n",
    "position = int(input(\"Enter position: \"))\n",
    "\n",
    "\n",
    "# Repetir el proceso count veces\n",
    "for i in range(count):\n",
    "    # Leer el contenido HTML de la URL\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    \n",
    "    # Analizar el HTML con BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Encontrar todas las etiquetas <a>\n",
    "    a_tags = soup.find_all('a')\n",
    "    \n",
    "    # Extraer la URL de la posición especificada\n",
    "    url = a_tags[position - 1]['href']\n",
    "    \n",
    "    # Mostrar la URL que se está recuperando\n",
    "    print(\"Retrieving:\", url)\n",
    "\n",
    "# Imprimir la última URL visitada (último nombre recuperado)\n",
    "print(\"Last name retrieved:\", url.split('_')[-1].split('.')[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ejercicio ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extracting Data from XML\n",
    "\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/geoxml.py. The program will prompt for a URL, read the XML data from that URL using urllib and then parse and extract the comment counts from the XML data, compute the sum of the numbers in the file.\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "    Sample data: http://py4e-data.dr-chuck.net/comments_42.xml (Sum=2553)\n",
    "    Actual data: http://py4e-data.dr-chuck.net/comments_1968704.xml (Sum ends with 13)\n",
    "\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "\n",
    "Data Format and Approach\n",
    "\n",
    "The data consists of a number of names and comment counts in XML as follows:\n",
    "\n",
    "<comment>\n",
    "  <name>Matthias</name>\n",
    "  <count>97</count>\n",
    "</comment>\n",
    "\n",
    "You are to look through all the <comment> tags and find the <count> values sum the numbers. The closest sample code that shows how to parse XML is geoxml.py. But since the nesting of the elements in our data is different than the data we are parsing in that sample code you will have to make real changes to the code.\n",
    "\n",
    "To make the code a little simpler, you can use an XPath selector string to look through the entire tree of XML for any tag named 'count' with the following line of code:\n",
    "\n",
    "counts = tree.findall('.//count')\n",
    "\n",
    "Take a look at the Python ElementTree documentation and look for the supported XPath syntax for details. You could also work from the top of the XML down to the comments node and then loop through the child nodes of the comments node.\n",
    "\n",
    "Sample Execution\n",
    "\n",
    "$ python3 solution.py\n",
    "Enter location: http://py4e-data.dr-chuck.net/comments_42.xml\n",
    "Retrieving http://py4e-data.dr-chuck.net/comments_42.xml\n",
    "Retrieved 4189 characters\n",
    "Count: 50\n",
    "Sum: 2...\n",
    "\n",
    "Turning in the Assignment\n",
    "Enter the sum from the actual data and your Python code below:\n",
    "Sum: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/comments_1968704.xml\n",
      "Retrieved 4217 characters\n",
      "Count: 50\n",
      "Sum: 2113\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Solicitar la URL al usuario\n",
    "url = input(\"Enter location: \")\n",
    "\n",
    "#if len(url) < 1:\n",
    " #   url = \"http://py4e-data.dr-chuck.net/comments_42.xml\"\n",
    "\n",
    "\n",
    "if len(url) < 1:\n",
    "    url = \"http://py4e-data.dr-chuck.net/comments_1968704.xml\"\n",
    "\n",
    "\n",
    "# Leer el archivo XML de la URL\n",
    "print(f\"Retrieving {url}\")\n",
    "response = urllib.request.urlopen(url)\n",
    "data = response.read()\n",
    "print(f\"Retrieved {len(data)} characters\")\n",
    "\n",
    "# Analizar el archivo XML\n",
    "tree = ET.fromstring(data)\n",
    "\n",
    "# Encontrar todos los elementos <count> usando XPath\n",
    "counts = tree.findall('.//count')\n",
    "\n",
    "# Sumar los valores de <count>\n",
    "sum_count = sum(int(count.text) for count in counts)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Count: {len(counts)}\")\n",
    "print(f\"Sum: {sum_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Extracting Data from JSON\n",
    "\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/json2.py. The program will prompt for a URL, read the JSON data from that URL using urllib and then parse and extract the comment counts from the JSON data, compute the sum of the numbers in the file and enter the sum below:\n",
    "\n",
    "We provide two files for this assignment. One is a sample file where we give you the sum for your testing and the other is the actual data you need to process for the assignment.\n",
    "\n",
    "    Sample data: http://py4e-data.dr-chuck.net/comments_42.json (Sum=2553)\n",
    "    Actual data: http://py4e-data.dr-chuck.net/comments_1968705.json (Sum ends with 40)\n",
    "\n",
    "You do not need to save these files to your folder since your program will read the data directly from the URL. Note: Each student will have a distinct data url for the assignment - so only use your own data url for analysis.\n",
    "\n",
    "Data Format\n",
    "\n",
    "The data consists of a number of names and comment counts in JSON as follows:\n",
    "\n",
    "{\n",
    "  comments: [\n",
    "    {\n",
    "      name: \"Matthias\"\n",
    "      count: 97\n",
    "    },\n",
    "    {\n",
    "      name: \"Geomer\"\n",
    "      count: 97\n",
    "    }\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "The closest sample code that shows how to parse JSON and extract a list is json2.py. You might also want to look at geoxml.py to see how to prompt for a URL and retrieve data from a URL.\n",
    "\n",
    "Sample Execution\n",
    "\n",
    "$ python3 solution.py\n",
    "Enter location: http://py4e-data.dr-chuck.net/comments_42.json\n",
    "Retrieving http://py4e-data.dr-chuck.net/comments_42.json\n",
    "Retrieved 2733 characters\n",
    "Count: 50\n",
    "Sum: 2...\n",
    "\n",
    "Turning in the Assignment\n",
    "Enter the sum from the actual data and your Python code below:\n",
    "Sum: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 2640\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# Pedir la URL al usuario\n",
    "url = input(\"Enter location: \")\n",
    "\n",
    "#if len(url) < 1:\n",
    "#    url = \"http://py4e-data.dr-chuck.net/comments_42.json\"\n",
    "\n",
    "\n",
    "if len(url) < 1:\n",
    "    url = \"http://py4e-data.dr-chuck.net/comments_1968705.json\"\n",
    "\n",
    "\n",
    "# Descargar los datos JSON\n",
    "response = urllib.request.urlopen(url)\n",
    "data = response.read().decode()\n",
    "\n",
    "# Parsear los datos JSON\n",
    "json_data = json.loads(data)\n",
    "\n",
    "# Extraer la lista de comentarios\n",
    "comments = json_data['comments']\n",
    "\n",
    "# Sumar los valores de count\n",
    "total_sum = 0\n",
    "for comment in comments:\n",
    "    total_sum += comment['count']\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(f\"Sum: {total_sum}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Calling a JSON API\n",
    "In this assignment you will write a Python program somewhat similar to http://www.py4e.com/code3/opengeo.py. The program will prompt for a location, contact a web service and retrieve JSON for the web service and parse that data, and retrieve the first plus_code from the JSON. An Open Location Code is a textual identifier that is another form of address based on the location of the address.\n",
    "\n",
    "API End Points\n",
    "\n",
    "To complete this assignment, you should use this API endpoint that has a static subset of the Open Street Map Data.\n",
    "\n",
    "http://py4e-data.dr-chuck.net/opengeo?\n",
    "\n",
    "This API also has no rate limit so you can test as often as you like. If you visit the URL with no parameters, you get \"No address...\" response.\n",
    "\n",
    "To call the API, you need to provide the address that you are requesting as the q= parameter that is properly URL encoded using the urllib.parse.urlencode() function as shown in http://www.py4e.com/code3/opengeo.py\n",
    "\n",
    "Test Data / Sample Execution\n",
    "\n",
    "You can test to see if your program is working with a location of \"South Federal University\" which will have a plus_code of \"6FV8QPRJ+VQ\".\n",
    "\n",
    "$ python solution.py\n",
    "Enter location: South Federal University\n",
    "Retrieving http://...\n",
    "Retrieved 1466 characters\n",
    "Plus code 6FV8QPRJ+VQ\n",
    "\n",
    "Turn In\n",
    "\n",
    "Please run your program to find the plus_code for this location:\n",
    "\n",
    "Saint-Petersburg Polytechnic Univesity\n",
    "\n",
    "Make sure to enter the name and case exactly as above and enter the plus_code and your Python code below. Hint: The first five characters of the plus_code are \"85GP2 ...\"\n",
    "\n",
    "Make sure to retreive the data from the URL specified above and not the normal Google API. Your program should work with the Google API - but the plus_code may not match for this assignment.\n",
    "plus_code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving http://py4e-data.dr-chuck.net/opengeo?q=South+Federal+University\n",
      "Retrieved 1465 characters\n",
      "Plus code: 6FV8QPRJ+VQ\n",
      "Retrieving http://py4e-data.dr-chuck.net/opengeo?q=South+Federal+University\n",
      "Retrieved 1465 characters\n",
      "Plus code: 6FV8QPRJ+VQ\n",
      "Retrieving http://py4e-data.dr-chuck.net/opengeo?q=Saint-Petersburg+Polytechnic+Univesity\n",
      "Retrieved 1323 characters\n",
      "Plus code: 85GP2QW3+XC\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse\n",
    "import json, ssl\n",
    "\n",
    "# API endpoint\n",
    "serviceurl = 'http://py4e-data.dr-chuck.net/opengeo?'\n",
    "\n",
    "# Ignorar errores de certificados SSL\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "while True:\n",
    "    address = input('Enter location: ')\n",
    "    if len(address) < 1: \n",
    "        break\n",
    "\n",
    "    parms = dict()\n",
    "    parms['q'] = address\n",
    "    url = serviceurl + urllib.parse.urlencode(parms)\n",
    "\n",
    "    print('Retrieving', url)\n",
    "    uh = urllib.request.urlopen(url, context=ctx)\n",
    "    data = uh.read().decode()\n",
    "    print('Retrieved', len(data), 'characters')\n",
    "\n",
    "    # Cargar JSON\n",
    "    try:\n",
    "        js = json.loads(data)\n",
    "    except:\n",
    "        print('Failed to parse JSON')\n",
    "        continue\n",
    "\n",
    "    # Verificar si 'features' está en la respuesta JSON\n",
    "    if 'features' not in js:\n",
    "        print('No features found')\n",
    "        continue\n",
    "\n",
    "    # Extraer plus_code\n",
    "    plus_code = None\n",
    "    if len(js['features']) > 0:\n",
    "        plus_code = js['features'][0]['properties'].get('plus_code')\n",
    "        \n",
    "    if plus_code:\n",
    "        print(f\"Plus code: {plus_code}\")\n",
    "    else:\n",
    "        print('Plus code not found')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
